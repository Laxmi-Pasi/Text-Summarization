{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hGocFhAaqVBl",
   "metadata": {
    "id": "hGocFhAaqVBl"
   },
   "source": [
    "# **Import necessary** **libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c715365",
   "metadata": {
    "id": "7c715365"
   },
   "outputs": [],
   "source": [
    "import os  # Used for interacting with the operating system\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize  # Tokenization functions from NLTK\n",
    "from nltk.corpus import stopwords  # Stopwords from NLTK for text processing\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "import math  # Math library for mathematical functions\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zX88Pfm7q8wO",
   "metadata": {
    "id": "zX88Pfm7q8wO"
   },
   "source": [
    "# **Mount Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "zUnWXxXNl4g7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUnWXxXNl4g7",
    "outputId": "6964e24e-a1f5-4286-895b-67857100b083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "MlkA6r1nsB-9",
   "metadata": {
    "id": "MlkA6r1nsB-9"
   },
   "outputs": [],
   "source": [
    "def lexrank_summarizer(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    text = file.read()\n",
    "\n",
    "    text = text.strip().replace('\\n', ' ')\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\b\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b', '', text)\n",
    "\n",
    "    # Remove times\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{1,2}(:\\d{1,2})?\\b', '', text)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def token_lower(sentence):\n",
    "        # Convert the sentence to lowercase\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # Replace '$' with 'dollar'\n",
    "        sentence = re.sub(\"[$]\", \"dollar \", sentence)\n",
    "\n",
    "        # Replace '£' with 'pound'\n",
    "        sentence = re.sub(\"[£]\", \"pound \", sentence)\n",
    "\n",
    "        # Replace '%' with 'percent'\n",
    "        sentence = re.sub(\"[%]\", \" percent\", sentence)\n",
    "\n",
    "        # Remove characters other than alphanumeric, '?', '!', '.', ',', \"'\", and '-'\n",
    "        sentence = re.sub(r\"[^a-zA-Z0-9?!.,’-]\", ' ', sentence)\n",
    "\n",
    "        # Add spaces around '?', '!', and '.'\n",
    "        sentence = re.sub(r\"([?!])\", r\" \\1 \", sentence)\n",
    "\n",
    "        # Add space after a lowercase letter followed by '.', ','\n",
    "        sentence = re.sub(r'([a-z])(?=[.,])', r'\\1 ', sentence)\n",
    "\n",
    "        # Replace multiple spaces with a single space\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "\n",
    "        # Add a full stop at the end of the sentence\n",
    "        sentence += \".\"\n",
    "\n",
    "        # Tokenize the sentence using NLTK's word_tokenize\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        # Filter out stopwords and non-alphabetic words, and convert to lowercase\n",
    "        filtered_words = [word.lower() for word in words if (word not in stop_words and word.isalpha())]\n",
    "\n",
    "        return filtered_words\n",
    "\n",
    "    tok_fil_sent = list(map(token_lower, sentences))\n",
    "    num_nodes = len(tok_fil_sent)\n",
    "\n",
    "    def idf(tok_fil_sent):\n",
    "        word_idf = {}\n",
    "        sent_set = []\n",
    "        words = set()\n",
    "        num_sent = len(tok_fil_sent)\n",
    "        for i in range(num_sent):\n",
    "            sent_set += [set(tok_fil_sent[i])]\n",
    "            words |= sent_set[i]\n",
    "        words = list(words)\n",
    "        for word in words:\n",
    "            word_idf[word] = math.log(float(num_sent) / sum([1 for i in range(num_sent) if word in sent_set[i]]))\n",
    "        return word_idf\n",
    "\n",
    "    word_idf = idf(tok_fil_sent)\n",
    "\n",
    "    def idf_mod_cos(sent1, sent2, word_idf):\n",
    "        sent1_dict = {}\n",
    "        sent2_dict = {}\n",
    "        for word in sent1:\n",
    "            if word in sent1_dict:\n",
    "                sent1_dict[word] += 1\n",
    "            else:\n",
    "                sent1_dict[word] = 1\n",
    "        for word in sent2:\n",
    "            if word in sent2_dict:\n",
    "                sent2_dict[word] += 1\n",
    "            else:\n",
    "                sent2_dict[word] = 1\n",
    "\n",
    "        common_words = set(sent1_dict.keys()) & set(sent2_dict.keys())\n",
    "\n",
    "        if not common_words:\n",
    "            return 0.0\n",
    "\n",
    "        numerator = sum(sent1_dict[word] * sent2_dict[word] * word_idf[word] * word_idf[word] for word in common_words)\n",
    "\n",
    "        sum_squared_sent1 = sum((sent1_dict[word] * word_idf[word]) ** 2 for word in sent1_dict)\n",
    "        sum_squared_sent2 = sum((sent2_dict[word] * word_idf[word]) ** 2 for word in sent2_dict)\n",
    "\n",
    "        denominator = math.sqrt(sum_squared_sent1) * math.sqrt(sum_squared_sent2)\n",
    "\n",
    "        if denominator == 0.0:\n",
    "            return 0.0\n",
    "\n",
    "        similarity = numerator / denominator\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    graph = np.zeros((num_nodes, num_nodes))\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            graph[i, j] = idf_mod_cos(tok_fil_sent[i], tok_fil_sent[j], word_idf)\n",
    "            graph[j, i] = graph[i, j]\n",
    "\n",
    "    node_weights = np.ones(num_nodes)\n",
    "\n",
    "    def text_rank_sent(graph, node_weights, d=0.85, iter=20):\n",
    "        weight_sum = np.sum(graph, axis=0)\n",
    "        while iter > 0:\n",
    "            for i in range(len(node_weights)):\n",
    "                temp = 0.0\n",
    "                for j in range(len(node_weights)):\n",
    "                    temp += graph[i, j] * node_weights[j] / weight_sum[j]\n",
    "                node_weights[i] = 1 - d + (d * temp)\n",
    "            iter -= 1\n",
    "\n",
    "    text_rank_sent(graph, node_weights)\n",
    "\n",
    "    top_k = 10  # assuming you want to print the top 10 sentences\n",
    "    top_index = [i for i, j in sorted(enumerate(node_weights), key=lambda x: x[1], reverse=True)[:top_k]]\n",
    "    top_sentences = [sentences[i] for i in top_index]\n",
    "\n",
    "    # Check if there are top sentences to join\n",
    "    if top_sentences:\n",
    "        # Join top sentences with a single space and add a period at the end\n",
    "        ext_summary = \" \".join(top_sentences) + \".\"\n",
    "    else:\n",
    "        # Handle the case where there are no top sentences\n",
    "        ext_summary = \"\"\n",
    "\n",
    "    return ext_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243d54a3",
   "metadata": {
    "id": "243d54a3"
   },
   "outputs": [],
   "source": [
    "# Specify the folder paths\n",
    "annual_report_folder = 'fns2020_dataset/validation/annual_reports'\n",
    "output_folder = 'output_summaries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29037014",
   "metadata": {
    "id": "29037014"
   },
   "outputs": [],
   "source": [
    "# Ensure the output folder exists, create if not\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mgsCJbcHpj3K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgsCJbcHpj3K",
    "outputId": "2ee5c1e4-dda8-48b7-c8e4-b22896121382"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21f7935f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21f7935f",
    "outputId": "c3fb71cf-ce9a-4cf6-c712-5dcda9d5246d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10827/1279677729.py:123: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  temp += graph[i, j] * node_weights[j] / weight_sum[j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for 31939.txt written to output_summaries/summary_31939.txt\n",
      "Summary for 31839.txt written to output_summaries/summary_31839.txt\n",
      "Summary for 32886.txt written to output_summaries/summary_32886.txt\n",
      "Summary for 31333.txt written to output_summaries/summary_31333.txt\n",
      "Summary for 32602.txt written to output_summaries/summary_32602.txt\n",
      "Summary for 33038.txt written to output_summaries/summary_33038.txt\n",
      "Summary for 31654.txt written to output_summaries/summary_31654.txt\n",
      "Summary for 31857.txt written to output_summaries/summary_31857.txt\n",
      "Summary for 31620.txt written to output_summaries/summary_31620.txt\n",
      "Summary for 30894.txt written to output_summaries/summary_30894.txt\n",
      "Summary for 31033.txt written to output_summaries/summary_31033.txt\n",
      "Summary for 31007.txt written to output_summaries/summary_31007.txt\n",
      "Summary for 31224.txt written to output_summaries/summary_31224.txt\n",
      "Summary for 32143.txt written to output_summaries/summary_32143.txt\n",
      "Summary for 31037.txt written to output_summaries/summary_31037.txt\n",
      "Summary for 30830.txt written to output_summaries/summary_30830.txt\n",
      "Summary for 32599.txt written to output_summaries/summary_32599.txt\n",
      "Summary for 30943.txt written to output_summaries/summary_30943.txt\n",
      "Summary for 32355.txt written to output_summaries/summary_32355.txt\n",
      "Summary for 31784.txt written to output_summaries/summary_31784.txt\n",
      "Summary for 32340.txt written to output_summaries/summary_32340.txt\n",
      "Summary for 32021.txt written to output_summaries/summary_32021.txt\n",
      "Summary for 31146.txt written to output_summaries/summary_31146.txt\n",
      "Summary for 31015.txt written to output_summaries/summary_31015.txt\n",
      "Summary for 33083.txt written to output_summaries/summary_33083.txt\n",
      "Summary for 31969.txt written to output_summaries/summary_31969.txt\n",
      "Summary for 30954.txt written to output_summaries/summary_30954.txt\n",
      "Summary for 32286.txt written to output_summaries/summary_32286.txt\n",
      "Summary for 31114.txt written to output_summaries/summary_31114.txt\n",
      "Summary for 31846.txt written to output_summaries/summary_31846.txt\n",
      "Summary for 30962.txt written to output_summaries/summary_30962.txt\n",
      "Summary for 31315.txt written to output_summaries/summary_31315.txt\n",
      "Summary for 32020.txt written to output_summaries/summary_32020.txt\n",
      "Summary for 32107.txt written to output_summaries/summary_32107.txt\n",
      "Summary for 32098.txt written to output_summaries/summary_32098.txt\n",
      "Summary for 31329.txt written to output_summaries/summary_31329.txt\n",
      "Summary for 31274.txt written to output_summaries/summary_31274.txt\n",
      "Summary for 30968.txt written to output_summaries/summary_30968.txt\n",
      "Summary for 31979.txt written to output_summaries/summary_31979.txt\n",
      "Summary for 31061.txt written to output_summaries/summary_31061.txt\n",
      "Summary for 31165.txt written to output_summaries/summary_31165.txt\n",
      "Summary for 32854.txt written to output_summaries/summary_32854.txt\n",
      "Summary for 31634.txt written to output_summaries/summary_31634.txt\n",
      "Summary for 31058.txt written to output_summaries/summary_31058.txt\n",
      "Summary for 30920.txt written to output_summaries/summary_30920.txt\n",
      "Summary for 31866.txt written to output_summaries/summary_31866.txt\n",
      "Summary for 31938.txt written to output_summaries/summary_31938.txt\n",
      "Summary for 30790.txt written to output_summaries/summary_30790.txt\n",
      "Summary for 31260.txt written to output_summaries/summary_31260.txt\n",
      "Summary for 30966.txt written to output_summaries/summary_30966.txt\n",
      "Summary for 31053.txt written to output_summaries/summary_31053.txt\n",
      "Summary for 32155.txt written to output_summaries/summary_32155.txt\n",
      "Summary for 30940.txt written to output_summaries/summary_30940.txt\n",
      "Summary for 31608.txt written to output_summaries/summary_31608.txt\n",
      "Summary for 32034.txt written to output_summaries/summary_32034.txt\n",
      "Summary for 32349.txt written to output_summaries/summary_32349.txt\n",
      "Summary for 30960.txt written to output_summaries/summary_30960.txt\n",
      "Summary for 31358.txt written to output_summaries/summary_31358.txt\n",
      "Summary for 32734.txt written to output_summaries/summary_32734.txt\n",
      "Summary for 30952.txt written to output_summaries/summary_30952.txt\n",
      "Summary for 32553.txt written to output_summaries/summary_32553.txt\n",
      "Summary for 31628.txt written to output_summaries/summary_31628.txt\n",
      "Summary for 32541.txt written to output_summaries/summary_32541.txt\n",
      "Summary for 33036.txt written to output_summaries/summary_33036.txt\n",
      "Summary for 31509.txt written to output_summaries/summary_31509.txt\n",
      "Summary for 32170.txt written to output_summaries/summary_32170.txt\n",
      "Summary for 30887.txt written to output_summaries/summary_30887.txt\n",
      "Summary for 33027.txt written to output_summaries/summary_33027.txt\n",
      "Summary for 31950.txt written to output_summaries/summary_31950.txt\n",
      "Summary for 31038.txt written to output_summaries/summary_31038.txt\n",
      "Summary for 31292.txt written to output_summaries/summary_31292.txt\n",
      "Summary for 30841.txt written to output_summaries/summary_30841.txt\n",
      "Summary for 31478.txt written to output_summaries/summary_31478.txt\n",
      "Summary for 32859.txt written to output_summaries/summary_32859.txt\n",
      "Summary for 32343.txt written to output_summaries/summary_32343.txt\n",
      "Summary for 32141.txt written to output_summaries/summary_32141.txt\n",
      "Summary for 30948.txt written to output_summaries/summary_30948.txt\n",
      "Summary for 32139.txt written to output_summaries/summary_32139.txt\n",
      "Summary for 31452.txt written to output_summaries/summary_31452.txt\n",
      "Summary for 31201.txt written to output_summaries/summary_31201.txt\n",
      "Summary for 33097.txt written to output_summaries/summary_33097.txt\n",
      "Summary for 31831.txt written to output_summaries/summary_31831.txt\n",
      "Summary for 32197.txt written to output_summaries/summary_32197.txt\n",
      "Summary for 31681.txt written to output_summaries/summary_31681.txt\n",
      "Summary for 31817.txt written to output_summaries/summary_31817.txt\n",
      "Summary for 31254.txt written to output_summaries/summary_31254.txt\n",
      "Summary for 32688.txt written to output_summaries/summary_32688.txt\n",
      "Summary for 30856.txt written to output_summaries/summary_30856.txt\n",
      "Summary for 30796.txt written to output_summaries/summary_30796.txt\n",
      "Summary for 31811.txt written to output_summaries/summary_31811.txt\n",
      "Summary for 30965.txt written to output_summaries/summary_30965.txt\n",
      "Summary for 31872.txt written to output_summaries/summary_31872.txt\n",
      "Summary for 31031.txt written to output_summaries/summary_31031.txt\n",
      "Summary for 30850.txt written to output_summaries/summary_30850.txt\n",
      "Summary for 30821.txt written to output_summaries/summary_30821.txt\n",
      "Summary for 31014.txt written to output_summaries/summary_31014.txt\n",
      "Summary for 30855.txt written to output_summaries/summary_30855.txt\n",
      "Summary for 31617.txt written to output_summaries/summary_31617.txt\n",
      "Summary for 32129.txt written to output_summaries/summary_32129.txt\n",
      "Summary for 32190.txt written to output_summaries/summary_32190.txt\n",
      "Summary for 31040.txt written to output_summaries/summary_31040.txt\n",
      "Summary for 31983.txt written to output_summaries/summary_31983.txt\n",
      "Summary for 31076.txt written to output_summaries/summary_31076.txt\n",
      "Summary for 32384.txt written to output_summaries/summary_32384.txt\n",
      "Summary for 33130.txt written to output_summaries/summary_33130.txt\n",
      "Summary for 32616.txt written to output_summaries/summary_32616.txt\n",
      "Summary for 32810.txt written to output_summaries/summary_32810.txt\n",
      "Summary for 32135.txt written to output_summaries/summary_32135.txt\n",
      "Summary for 31546.txt written to output_summaries/summary_31546.txt\n",
      "Summary for 32771.txt written to output_summaries/summary_32771.txt\n",
      "Summary for 31271.txt written to output_summaries/summary_31271.txt\n",
      "Summary for 31987.txt written to output_summaries/summary_31987.txt\n",
      "Summary for 32531.txt written to output_summaries/summary_32531.txt\n",
      "Summary for 32392.txt written to output_summaries/summary_32392.txt\n",
      "Summary for 31008.txt written to output_summaries/summary_31008.txt\n",
      "Summary for 32180.txt written to output_summaries/summary_32180.txt\n",
      "Summary for 30782.txt written to output_summaries/summary_30782.txt\n",
      "Summary for 30886.txt written to output_summaries/summary_30886.txt\n",
      "Summary for 31959.txt written to output_summaries/summary_31959.txt\n",
      "Summary for 32538.txt written to output_summaries/summary_32538.txt\n",
      "Summary for 32874.txt written to output_summaries/summary_32874.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for 32004.txt written to output_summaries/summary_32004.txt\n",
      "Summary for 33037.txt written to output_summaries/summary_33037.txt\n",
      "Summary for 30950.txt written to output_summaries/summary_30950.txt\n",
      "Summary for 32560.txt written to output_summaries/summary_32560.txt\n",
      "Summary for 31319.txt written to output_summaries/summary_31319.txt\n",
      "Summary for 31023.txt written to output_summaries/summary_31023.txt\n",
      "Summary for 30820.txt written to output_summaries/summary_30820.txt\n",
      "Summary for 31940.txt written to output_summaries/summary_31940.txt\n",
      "Summary for 31948.txt written to output_summaries/summary_31948.txt\n",
      "Summary for 31508.txt written to output_summaries/summary_31508.txt\n",
      "Summary for 30783.txt written to output_summaries/summary_30783.txt\n",
      "Summary for 32032.txt written to output_summaries/summary_32032.txt\n",
      "Summary for 31484.txt written to output_summaries/summary_31484.txt\n",
      "Summary for 31026.txt written to output_summaries/summary_31026.txt\n",
      "Summary for 32378.txt written to output_summaries/summary_32378.txt\n",
      "Summary for 31540.txt written to output_summaries/summary_31540.txt\n",
      "Summary for 31474.txt written to output_summaries/summary_31474.txt\n",
      "Summary for 31623.txt written to output_summaries/summary_31623.txt\n",
      "Summary for 31336.txt written to output_summaries/summary_31336.txt\n",
      "Summary for 31203.txt written to output_summaries/summary_31203.txt\n",
      "Summary for 32042.txt written to output_summaries/summary_32042.txt\n",
      "Summary for 31824.txt written to output_summaries/summary_31824.txt\n",
      "Summary for 30913.txt written to output_summaries/summary_30913.txt\n",
      "Summary for 32140.txt written to output_summaries/summary_32140.txt\n",
      "Summary for 31655.txt written to output_summaries/summary_31655.txt\n",
      "Summary for 31632.txt written to output_summaries/summary_31632.txt\n",
      "Summary for 32229.txt written to output_summaries/summary_32229.txt\n",
      "Summary for 32651.txt written to output_summaries/summary_32651.txt\n",
      "Summary for 32936.txt written to output_summaries/summary_32936.txt\n",
      "Summary for 32380.txt written to output_summaries/summary_32380.txt\n",
      "Summary for 30897.txt written to output_summaries/summary_30897.txt\n",
      "Summary for 30967.txt written to output_summaries/summary_30967.txt\n",
      "Summary for 32272.txt written to output_summaries/summary_32272.txt\n",
      "Summary for 31022.txt written to output_summaries/summary_31022.txt\n",
      "Summary for 31828.txt written to output_summaries/summary_31828.txt\n",
      "Summary for 32556.txt written to output_summaries/summary_32556.txt\n",
      "Summary for 31568.txt written to output_summaries/summary_31568.txt\n",
      "Summary for 31012.txt written to output_summaries/summary_31012.txt\n",
      "Summary for 31105.txt written to output_summaries/summary_31105.txt\n",
      "Summary for 32623.txt written to output_summaries/summary_32623.txt\n",
      "Summary for 32408.txt written to output_summaries/summary_32408.txt\n",
      "Summary for 33057.txt written to output_summaries/summary_33057.txt\n",
      "Summary for 32177.txt written to output_summaries/summary_32177.txt\n",
      "Summary for 31766.txt written to output_summaries/summary_31766.txt\n",
      "Summary for 32555.txt written to output_summaries/summary_32555.txt\n",
      "Summary for 32414.txt written to output_summaries/summary_32414.txt\n",
      "Summary for 30924.txt written to output_summaries/summary_30924.txt\n",
      "Summary for 31610.txt written to output_summaries/summary_31610.txt\n",
      "Summary for 32926.txt written to output_summaries/summary_32926.txt\n",
      "Summary for 32424.txt written to output_summaries/summary_32424.txt\n",
      "Summary for 30904.txt written to output_summaries/summary_30904.txt\n",
      "Summary for 32156.txt written to output_summaries/summary_32156.txt\n",
      "Summary for 31664.txt written to output_summaries/summary_31664.txt\n",
      "Summary for 31661.txt written to output_summaries/summary_31661.txt\n",
      "Summary for 30991.txt written to output_summaries/summary_30991.txt\n",
      "Summary for 31595.txt written to output_summaries/summary_31595.txt\n",
      "Summary for 32149.txt written to output_summaries/summary_32149.txt\n",
      "Summary for 32134.txt written to output_summaries/summary_32134.txt\n",
      "Summary for 30854.txt written to output_summaries/summary_30854.txt\n",
      "Summary for 32236.txt written to output_summaries/summary_32236.txt\n",
      "Summary for 31469.txt written to output_summaries/summary_31469.txt\n",
      "Summary for 32389.txt written to output_summaries/summary_32389.txt\n",
      "Summary for 32220.txt written to output_summaries/summary_32220.txt\n",
      "Summary for 32196.txt written to output_summaries/summary_32196.txt\n",
      "Summary for 32176.txt written to output_summaries/summary_32176.txt\n",
      "Summary for 31926.txt written to output_summaries/summary_31926.txt\n",
      "Summary for 31757.txt written to output_summaries/summary_31757.txt\n",
      "Summary for 32217.txt written to output_summaries/summary_32217.txt\n",
      "Summary for 31567.txt written to output_summaries/summary_31567.txt\n",
      "Summary for 32809.txt written to output_summaries/summary_32809.txt\n",
      "Summary for 32879.txt written to output_summaries/summary_32879.txt\n",
      "Summary for 31013.txt written to output_summaries/summary_31013.txt\n",
      "Summary for 32848.txt written to output_summaries/summary_32848.txt\n",
      "Summary for 30815.txt written to output_summaries/summary_30815.txt\n",
      "Summary for 31063.txt written to output_summaries/summary_31063.txt\n",
      "Summary for 31316.txt written to output_summaries/summary_31316.txt\n",
      "Summary for 31042.txt written to output_summaries/summary_31042.txt\n",
      "Summary for 31036.txt written to output_summaries/summary_31036.txt\n",
      "Summary for 31290.txt written to output_summaries/summary_31290.txt\n",
      "Summary for 32150.txt written to output_summaries/summary_32150.txt\n",
      "Summary for 31506.txt written to output_summaries/summary_31506.txt\n",
      "Summary for 32051.txt written to output_summaries/summary_32051.txt\n",
      "Summary for 30947.txt written to output_summaries/summary_30947.txt\n",
      "Summary for 32562.txt written to output_summaries/summary_32562.txt\n",
      "Summary for 30778.txt written to output_summaries/summary_30778.txt\n",
      "Summary for 30852.txt written to output_summaries/summary_30852.txt\n",
      "Summary for 31601.txt written to output_summaries/summary_31601.txt\n",
      "Summary for 31843.txt written to output_summaries/summary_31843.txt\n",
      "Summary for 31127.txt written to output_summaries/summary_31127.txt\n",
      "Summary for 31517.txt written to output_summaries/summary_31517.txt\n",
      "Summary for 31424.txt written to output_summaries/summary_31424.txt\n",
      "Summary for 31462.txt written to output_summaries/summary_31462.txt\n",
      "Summary for 32376.txt written to output_summaries/summary_32376.txt\n",
      "Summary for 32067.txt written to output_summaries/summary_32067.txt\n",
      "Summary for 33090.txt written to output_summaries/summary_33090.txt\n",
      "Summary for 33029.txt written to output_summaries/summary_33029.txt\n",
      "Summary for 30777.txt written to output_summaries/summary_30777.txt\n",
      "Summary for 31512.txt written to output_summaries/summary_31512.txt\n",
      "Summary for 31050.txt written to output_summaries/summary_31050.txt\n",
      "Summary for 32557.txt written to output_summaries/summary_32557.txt\n",
      "Summary for 31914.txt written to output_summaries/summary_31914.txt\n",
      "Summary for 30813.txt written to output_summaries/summary_30813.txt\n",
      "Summary for 32865.txt written to output_summaries/summary_32865.txt\n",
      "Summary for 31646.txt written to output_summaries/summary_31646.txt\n",
      "Summary for 30922.txt written to output_summaries/summary_30922.txt\n",
      "Summary for 32168.txt written to output_summaries/summary_32168.txt\n",
      "Summary for 30895.txt written to output_summaries/summary_30895.txt\n",
      "Summary for 31513.txt written to output_summaries/summary_31513.txt\n",
      "Summary for 30785.txt written to output_summaries/summary_30785.txt\n",
      "Summary for 31059.txt written to output_summaries/summary_31059.txt\n",
      "Summary for 31220.txt written to output_summaries/summary_31220.txt\n",
      "Summary for 32583.txt written to output_summaries/summary_32583.txt\n",
      "Summary for 31834.txt written to output_summaries/summary_31834.txt\n",
      "Summary for 32359.txt written to output_summaries/summary_32359.txt\n",
      "Summary for 30808.txt written to output_summaries/summary_30808.txt\n",
      "Summary for 30849.txt written to output_summaries/summary_30849.txt\n",
      "Summary for 32097.txt written to output_summaries/summary_32097.txt\n",
      "Summary for 32183.txt written to output_summaries/summary_32183.txt\n",
      "Summary for 30819.txt written to output_summaries/summary_30819.txt\n",
      "Summary for 31202.txt written to output_summaries/summary_31202.txt\n",
      "Summary for 32609.txt written to output_summaries/summary_32609.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for 31005.txt written to output_summaries/summary_31005.txt\n",
      "Summary for 32543.txt written to output_summaries/summary_32543.txt\n",
      "Summary for 31034.txt written to output_summaries/summary_31034.txt\n",
      "Summary for 31879.txt written to output_summaries/summary_31879.txt\n",
      "Summary for 31573.txt written to output_summaries/summary_31573.txt\n",
      "Summary for 30800.txt written to output_summaries/summary_30800.txt\n",
      "Summary for 32071.txt written to output_summaries/summary_32071.txt\n",
      "Summary for 31399.txt written to output_summaries/summary_31399.txt\n",
      "Summary for 31265.txt written to output_summaries/summary_31265.txt\n",
      "Summary for 32185.txt written to output_summaries/summary_32185.txt\n",
      "Summary for 30838.txt written to output_summaries/summary_30838.txt\n",
      "Summary for 31970.txt written to output_summaries/summary_31970.txt\n",
      "Summary for 32717.txt written to output_summaries/summary_32717.txt\n",
      "Summary for 32072.txt written to output_summaries/summary_32072.txt\n",
      "Summary for 32849.txt written to output_summaries/summary_32849.txt\n",
      "Summary for 31699.txt written to output_summaries/summary_31699.txt\n",
      "Summary for 31101.txt written to output_summaries/summary_31101.txt\n",
      "Summary for 32025.txt written to output_summaries/summary_32025.txt\n",
      "Summary for 33035.txt written to output_summaries/summary_33035.txt\n",
      "Summary for 31889.txt written to output_summaries/summary_31889.txt\n",
      "Summary for 30779.txt written to output_summaries/summary_30779.txt\n",
      "Summary for 31854.txt written to output_summaries/summary_31854.txt\n",
      "Summary for 32248.txt written to output_summaries/summary_32248.txt\n",
      "Summary for 30982.txt written to output_summaries/summary_30982.txt\n",
      "Summary for 32116.txt written to output_summaries/summary_32116.txt\n",
      "Summary for 31492.txt written to output_summaries/summary_31492.txt\n",
      "Summary for 31650.txt written to output_summaries/summary_31650.txt\n",
      "Summary for 31074.txt written to output_summaries/summary_31074.txt\n",
      "Summary for 30874.txt written to output_summaries/summary_30874.txt\n",
      "Summary for 32113.txt written to output_summaries/summary_32113.txt\n",
      "Summary for 32163.txt written to output_summaries/summary_32163.txt\n",
      "Summary for 32773.txt written to output_summaries/summary_32773.txt\n",
      "Summary for 31705.txt written to output_summaries/summary_31705.txt\n",
      "Summary for 30899.txt written to output_summaries/summary_30899.txt\n",
      "Summary for 32480.txt written to output_summaries/summary_32480.txt\n",
      "Summary for 32074.txt written to output_summaries/summary_32074.txt\n",
      "Summary for 31659.txt written to output_summaries/summary_31659.txt\n",
      "Summary for 33053.txt written to output_summaries/summary_33053.txt\n",
      "Summary for 32648.txt written to output_summaries/summary_32648.txt\n",
      "Summary for 31851.txt written to output_summaries/summary_31851.txt\n",
      "Summary for 33039.txt written to output_summaries/summary_33039.txt\n",
      "Summary for 32954.txt written to output_summaries/summary_32954.txt\n",
      "Summary for 32157.txt written to output_summaries/summary_32157.txt\n",
      "Summary for 30901.txt written to output_summaries/summary_30901.txt\n",
      "Summary for 30866.txt written to output_summaries/summary_30866.txt\n",
      "Summary for 31755.txt written to output_summaries/summary_31755.txt\n",
      "Summary for 31383.txt written to output_summaries/summary_31383.txt\n",
      "Summary for 30941.txt written to output_summaries/summary_30941.txt\n",
      "Summary for 32561.txt written to output_summaries/summary_32561.txt\n",
      "Summary for 32684.txt written to output_summaries/summary_32684.txt\n",
      "Summary for 32657.txt written to output_summaries/summary_32657.txt\n",
      "Summary for 31618.txt written to output_summaries/summary_31618.txt\n",
      "Summary for 31109.txt written to output_summaries/summary_31109.txt\n",
      "Summary for 33098.txt written to output_summaries/summary_33098.txt\n",
      "Summary for 30817.txt written to output_summaries/summary_30817.txt\n",
      "Summary for 33154.txt written to output_summaries/summary_33154.txt\n",
      "Summary for 30907.txt written to output_summaries/summary_30907.txt\n",
      "Summary for 32939.txt written to output_summaries/summary_32939.txt\n",
      "Summary for 32545.txt written to output_summaries/summary_32545.txt\n",
      "Summary for 31942.txt written to output_summaries/summary_31942.txt\n",
      "Summary for 31677.txt written to output_summaries/summary_31677.txt\n",
      "Summary for 32792.txt written to output_summaries/summary_32792.txt\n",
      "Summary for 32482.txt written to output_summaries/summary_32482.txt\n",
      "Summary for 31633.txt written to output_summaries/summary_31633.txt\n",
      "Summary for 31363.txt written to output_summaries/summary_31363.txt\n",
      "Summary for 32548.txt written to output_summaries/summary_32548.txt\n",
      "Summary for 31769.txt written to output_summaries/summary_31769.txt\n",
      "Summary for 32966.txt written to output_summaries/summary_32966.txt\n",
      "Summary for 32061.txt written to output_summaries/summary_32061.txt\n",
      "Summary for 32148.txt written to output_summaries/summary_32148.txt\n",
      "Summary for 31955.txt written to output_summaries/summary_31955.txt\n",
      "Summary for 31844.txt written to output_summaries/summary_31844.txt\n",
      "Summary for 32333.txt written to output_summaries/summary_32333.txt\n",
      "Summary for 30902.txt written to output_summaries/summary_30902.txt\n",
      "Summary for 30816.txt written to output_summaries/summary_30816.txt\n",
      "Summary for 31440.txt written to output_summaries/summary_31440.txt\n",
      "Summary for 31204.txt written to output_summaries/summary_31204.txt\n",
      "Summary for 32179.txt written to output_summaries/summary_32179.txt\n",
      "Summary for 31589.txt written to output_summaries/summary_31589.txt\n",
      "Summary for 32437.txt written to output_summaries/summary_32437.txt\n",
      "Summary for 31631.txt written to output_summaries/summary_31631.txt\n",
      "Summary for 33054.txt written to output_summaries/summary_33054.txt\n",
      "Summary for 30795.txt written to output_summaries/summary_30795.txt\n",
      "Summary for 31685.txt written to output_summaries/summary_31685.txt\n",
      "Summary for 31242.txt written to output_summaries/summary_31242.txt\n",
      "Summary for 32825.txt written to output_summaries/summary_32825.txt\n",
      "Summary for 33070.txt written to output_summaries/summary_33070.txt\n",
      "Summary for 32223.txt written to output_summaries/summary_32223.txt\n",
      "Summary for 32345.txt written to output_summaries/summary_32345.txt\n",
      "Summary for 31984.txt written to output_summaries/summary_31984.txt\n",
      "Summary for 33155.txt written to output_summaries/summary_33155.txt\n",
      "Summary for 30927.txt written to output_summaries/summary_30927.txt\n",
      "Summary for 30781.txt written to output_summaries/summary_30781.txt\n",
      "Summary for 30822.txt written to output_summaries/summary_30822.txt\n",
      "Summary for 31850.txt written to output_summaries/summary_31850.txt\n",
      "Summary for 31306.txt written to output_summaries/summary_31306.txt\n",
      "Summary for 32136.txt written to output_summaries/summary_32136.txt\n",
      "Summary for 32165.txt written to output_summaries/summary_32165.txt\n",
      "Summary for 32144.txt written to output_summaries/summary_32144.txt\n",
      "Summary for 32044.txt written to output_summaries/summary_32044.txt\n",
      "Summary for 31855.txt written to output_summaries/summary_31855.txt\n",
      "Summary for 30888.txt written to output_summaries/summary_30888.txt\n",
      "Summary for 31648.txt written to output_summaries/summary_31648.txt\n",
      "Summary for 32964.txt written to output_summaries/summary_32964.txt\n",
      "Summary for 31514.txt written to output_summaries/summary_31514.txt\n",
      "Summary for 31604.txt written to output_summaries/summary_31604.txt\n",
      "Summary for 31794.txt written to output_summaries/summary_31794.txt\n",
      "Summary for 32597.txt written to output_summaries/summary_32597.txt\n",
      "Summary for 32186.txt written to output_summaries/summary_32186.txt\n",
      "Summary for 32540.txt written to output_summaries/summary_32540.txt\n",
      "Summary for 32092.txt written to output_summaries/summary_32092.txt\n",
      "Summary for 31064.txt written to output_summaries/summary_31064.txt\n",
      "Summary for 30946.txt written to output_summaries/summary_30946.txt\n",
      "Summary for 32452.txt written to output_summaries/summary_32452.txt\n",
      "Summary for 31678.txt written to output_summaries/summary_31678.txt\n",
      "Summary for 33018.txt written to output_summaries/summary_33018.txt\n",
      "Summary for 30858.txt written to output_summaries/summary_30858.txt\n",
      "Summary for 32394.txt written to output_summaries/summary_32394.txt\n",
      "Summary for 32693.txt written to output_summaries/summary_32693.txt\n",
      "Summary for 31277.txt written to output_summaries/summary_31277.txt\n",
      "Summary for 30903.txt written to output_summaries/summary_30903.txt\n"
     ]
    }
   ],
   "source": [
    "# Process each file in the annual report folder\n",
    "for file_name in os.listdir(annual_report_folder):  # Adjust the limit as needed\n",
    "    file_path = os.path.join(annual_report_folder, file_name)\n",
    "    summary = lexrank_summarizer(file_path)\n",
    "\n",
    "    # Write the summary to an output file\n",
    "    output_file_path = os.path.join(output_folder, f'summary_{file_name}')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.write(summary)\n",
    "\n",
    "    print(f'Summary for {file_name} written to {output_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa46e5e",
   "metadata": {
    "id": "7fa46e5e"
   },
   "source": [
    "# create CSV file for generated extractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec758501",
   "metadata": {
    "id": "ec758501"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Folder path\n",
    "folder_path = \"output_summaries\"\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "# Iterate over files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Check if the item in the folder is a file (not a subfolder)\n",
    "    if os.path.isfile(file_path):\n",
    "        # Read the content of the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Append data to the list\n",
    "        data.append((filename, content))\n",
    "\n",
    "# Create a DataFrame\n",
    "rd_extra_summ = pd.DataFrame(data, columns=[\"Filename\", \"Ext_Summary\"])\n",
    "\n",
    "# Set the index to be \"31839\"\n",
    "rd_extra_summ.set_index(\"Filename\", inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "rd_extra_summ.to_csv(\"Ext_Summary_50_files_lexRank.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d103910",
   "metadata": {
    "id": "0d103910",
    "outputId": "7a0ec387-a4d9-40cb-d6de-ef5054b9c53e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         Ext_Summary\n",
      "Filename                                                            \n",
      "summary_32044.txt  25546.04    9 August 2017  PM    Proof Six NCC...\n",
      "summary_33070.txt  Tellitstraight Reuters Annual Report and Form ...\n",
      "summary_32139.txt  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...\n",
      "summary_32480.txt  Yorkshire Water  Services Limited Annual Repor...\n",
      "summary_30817.txt  Annual Report 201 7 Strategic Report   What we...\n",
      "...                                                              ...\n",
      "summary_32540.txt  HAYWARD TYLER GROUP PLC     REPORT & ACCOUNTS ...\n",
      "summary_31618.txt  ITE Group plc  Annual Report and Accounts 2017...\n",
      "summary_31037.txt  ANNUAL REPORT Symphony Environmental Technolog...\n",
      "summary_32134.txt  ANNUAL REPORT AND   FINANCIAL STATEMENTS  2016...\n",
      "summary_30897.txt  Ubisense Group plc Annual Report 2017 Ubisense...\n",
      "\n",
      "[363 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "print(rd_extra_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "954dc9ba",
   "metadata": {
    "id": "954dc9ba"
   },
   "outputs": [],
   "source": [
    "rd_extra_summ['Numeric_Filename'] = rd_extra_summ.index.str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94bc9c2a",
   "metadata": {
    "id": "94bc9c2a",
    "outputId": "3de47694-4a99-495a-a593-10a197a2a761"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ext_Summary</th>\n",
       "      <th>Numeric_Filename</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary_32044.txt</th>\n",
       "      <td>25546.04    9 August 2017  PM    Proof Six NCC...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary_33070.txt</th>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary_32139.txt</th>\n",
       "      <td>ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary_32480.txt</th>\n",
       "      <td>Yorkshire Water  Services Limited Annual Repor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary_30817.txt</th>\n",
       "      <td>Annual Report 201 7 Strategic Report   What we...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Ext_Summary  \\\n",
       "Filename                                                               \n",
       "summary_32044.txt  25546.04    9 August 2017  PM    Proof Six NCC...   \n",
       "summary_33070.txt  Tellitstraight Reuters Annual Report and Form ...   \n",
       "summary_32139.txt  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...   \n",
       "summary_32480.txt  Yorkshire Water  Services Limited Annual Repor...   \n",
       "summary_30817.txt  Annual Report 201 7 Strategic Report   What we...   \n",
       "\n",
       "                  Numeric_Filename  \n",
       "Filename                            \n",
       "summary_32044.txt              NaN  \n",
       "summary_33070.txt              NaN  \n",
       "summary_32139.txt              NaN  \n",
       "summary_32480.txt              NaN  \n",
       "summary_30817.txt              NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_extra_summ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d34e4dc7",
   "metadata": {
    "id": "d34e4dc7",
    "outputId": "205ede8e-6d6c-4090-aec9-fdce26bd18a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at output_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "folder_path = 'output_summaries'\n",
    "output_csv_path = 'output_summary.csv'\n",
    "\n",
    "# Initialize CSV file\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['filename', 'extra_summary']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate through text files\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Extract the file number (assuming the filename is in the format '17.txt', '18.txt', etc.)\n",
    "            file_number = os.path.splitext(filename)[0]\n",
    "\n",
    "            # Read content from the file\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Write to CSV\n",
    "            writer.writerow({'filename': file_number, 'extra_summary': content})\n",
    "\n",
    "print(f'CSV file created at {output_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673e466b",
   "metadata": {
    "id": "673e466b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "726187ce",
   "metadata": {
    "id": "726187ce"
   },
   "outputs": [],
   "source": [
    "res_ext_summ = pd.read_csv(\"output_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c8db4cb",
   "metadata": {
    "id": "1c8db4cb",
    "outputId": "da9ab6fa-9c49-4574-e4fd-8e02102ebf0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extra_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summary_32044</td>\n",
       "      <td>25546.04    9 August 2017  PM    Proof Six NCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summary_33070</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_32139</td>\n",
       "      <td>ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary_32480</td>\n",
       "      <td>Yorkshire Water  Services Limited Annual Repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary_30817</td>\n",
       "      <td>Annual Report 201 7 Strategic Report   What we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                      extra_summary\n",
       "0  summary_32044  25546.04    9 August 2017  PM    Proof Six NCC...\n",
       "1  summary_33070  Tellitstraight Reuters Annual Report and Form ...\n",
       "2  summary_32139  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...\n",
       "3  summary_32480  Yorkshire Water  Services Limited Annual Repor...\n",
       "4  summary_30817  Annual Report 201 7 Strategic Report   What we..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ext_summ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdd0c8c5",
   "metadata": {
    "id": "bdd0c8c5",
    "outputId": "64b01066-c2a0-401e-c00c-641e0f5b9f83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25546.04    9 August 2017  PM    Proof Six NCC Group plc  Annual Report and Accounts  for the year ended 31 May 2017 NCC Group plc        Stock Code: NCC securing   tomorrow,   today Annual Report and Accounts for the year ended 31 May 2017 NCC AR2017 - proof 8-Front.indd   3     25546.04    9 August 2017  PM    Proof Six Why   we exist NCC Group is a global expert  in cyber security and risk  mitigation, working with  businesses to protect  their brand, data (including  intellectual property), value and  reputations against the ever- evolving threat landscape. The Group’s independence, knowledge,  experience and global footprint ensures that  NCC Group can help businesses identify,  assess, mitigate and respond to the risks they  face within this fluid and hostile environment. NCC Group is passionate about changing the  shape of the internet to make it safer and  revolutionising the way in which organisations  think about cyber security. NCC Group currently operates from over   30 offices across the UK, continental Europe,  North America, Australia, Canada, Singapore  and the United Arab Emirates. Visit us online at   NAVIGATING THE REPORT For further information within this   document and relevant page numbers Additional information available online BUSINESS OVERVIEW       Stock Code: NCC NCC AR2017 - proof 8-Front.indd   4     25546.04    9 August 2017  PM    Proof Six Financial   highlights (1)  z NCC Group operate in high growth markets  z Our expertise is highly valued by our customers  z We are at the forefront of thought leadership in cyber security  z NCC Group Escrow is an attractive niche business  z Self-help measures to improve margins through an updated Target  Operating Model and efficient business processes in both divisions  z In a highly fragmented market, NCC Group’s scale creates opportunity  for significant value creation through targeted acquisitions. Revenue  (£m) Operating Profit/Loss  (£m)   Adjusted EBIT  (£m) Adjusted EPS  (pence) 2015 133.7 209.1 244.5 2016 2017 2013 99.2 110.7 2014 2015 22.6 11.4 (53.4) 2016 2017 2013 19.8 24.1 2014 2015 26.4 38.4 27.6 2016 2017 2013 23.9 26.0 2014 2015 9.4 11.2 6.7 2016 2017 2013 8.4 9.3 2014 CONTENTS  LISTING HERE Investment   case BUSINESS OVERVIEW Financial highlights 1 Executive Chairman’s statement 2 Group at a glance 6 A day in the life… 14 STRATEGIC REPORT 16  Highlights 17 The strategic review and   target operating model 18 The market opportunity 20 Business model 24 Our strategy 26 Q&A with Chris Stone and Brian Tenner 28 Interim Chief Executive’s review 30 Group performance review for 2017 44 Principal risks and uncertainties 48 Corporate social responsibility 54 GOVERNANCE 58 Chairman’s letter •• 59 Governance framework 61 Board of Directors 62 Operations board 64 Board composition and division of  responsibilities 65 Shareholder relations 71 Audit committee report 74 Nomination committee report 82 Cyber security committee report 84 Remuneration committee report 86 Directors’ report 108 Directors’ responsibilities statement 112 FINANCIAL STATEMENTS 113 Independent auditor’s report 114 Consolidated income statement 118 Consolidated statement of   comprehensive income 119 Consolidated statement of   financial position 120 Consolidated statement of cash flows 122 Company statement of cash flows 124 Statements of changes of equity 125 Notes to the accounts 127 ADDITIONAL INFORMATION  Glossary of terms 170 Company information 172 Contents (1)  All from continuing operations   NCC Group plc Annual Report and Accounts for the year ended 31 May 2017 1 NCC AR2017 - proof 8-Front.indd   1     25546.04    9 August 2017  PM    Proof Six Introduction  My first statement to shareholders as  Executive Chairman reflects on two  strong but contradictory themes. Firstly,  the past year has been very challenging,  both operationally and financially. Business performance has fallen short  of expectations, we have outgrown some  of our business processes and controls,  and we have experienced significant  changes to our Board. Equally, and more importantly, it is  already clear that the years ahead  present significant upside opportunities. Strong value creation will result if we  effectively implement our new strategy  and successfully manage NCC Group  through the transitional period in which  we now find ourselves..'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ext_summ['extra_summary'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293bd8a",
   "metadata": {
    "id": "5293bd8a"
   },
   "source": [
    "# abstractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b5ba2d3",
   "metadata": {
    "id": "9b5ba2d3",
    "outputId": "f0d49efd-3a02-460e-81ee-339abd745f97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472115a8",
   "metadata": {
    "id": "472115a8"
   },
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ccfeed",
   "metadata": {
    "id": "53ccfeed",
    "outputId": "3563f2a5-ef25-40b7-bfe5-274a525fbc26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Load pre-trained PEGASUS model and tokenizer\n",
    "model_name = \"google/pegasus-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "757d4829",
   "metadata": {
    "id": "757d4829"
   },
   "outputs": [],
   "source": [
    "res_ext_summ['token_count'] = res_ext_summ['extra_summary'].apply(lambda x: len(tokenizer.encode(x, max_length=1024, truncation=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd8e4988",
   "metadata": {
    "id": "dd8e4988",
    "outputId": "4c7ef545-c17c-4610-f953-b905b5fd1926"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extra_summary</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summary_32044</td>\n",
       "      <td>25546.04    9 August 2017  PM    Proof Six NCC...</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summary_33070</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_32139</td>\n",
       "      <td>ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary_32480</td>\n",
       "      <td>Yorkshire Water  Services Limited Annual Repor...</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary_30817</td>\n",
       "      <td>Annual Report 201 7 Strategic Report   What we...</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                      extra_summary  \\\n",
       "0  summary_32044  25546.04    9 August 2017  PM    Proof Six NCC...   \n",
       "1  summary_33070  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2  summary_32139  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...   \n",
       "3  summary_32480  Yorkshire Water  Services Limited Annual Repor...   \n",
       "4  summary_30817  Annual Report 201 7 Strategic Report   What we...   \n",
       "\n",
       "   token_count  \n",
       "0          780  \n",
       "1          666  \n",
       "2          301  \n",
       "3          509  \n",
       "4          699  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ext_summ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d9bccad",
   "metadata": {
    "id": "2d9bccad",
    "outputId": "8430f24f-1e8f-42bd-c700-06f90ca4b688"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ext_summ['token_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0dfcb86",
   "metadata": {
    "id": "b0dfcb86",
    "outputId": "b95c019f-09cb-4a97-c269-25ee4aeed15a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ext_summ['token_count'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25e947f5",
   "metadata": {
    "id": "25e947f5"
   },
   "outputs": [],
   "source": [
    "# Tokenize and generate summary\n",
    "def generate_abs_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=512, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41604007",
   "metadata": {
    "id": "41604007"
   },
   "outputs": [],
   "source": [
    "# Apply the function to each row in 'extra_summary' column\n",
    "res_ext_summ['gen_abs_summary'] = res_ext_summ['extra_summary'].apply(generate_abs_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf8f16f",
   "metadata": {
    "id": "ccf8f16f",
    "outputId": "eabea4c1-e3b0-4e85-86b6-23384caeca9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extra_summary</th>\n",
       "      <th>token_count</th>\n",
       "      <th>gen_abs_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summary_32044</td>\n",
       "      <td>25546.04    9 August 2017  PM    Proof Six NCC...</td>\n",
       "      <td>780</td>\n",
       "      <td>25546.04 9 August 2017 PM Proof Six NCC Group ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summary_33070</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>666</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_32139</td>\n",
       "      <td>ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...</td>\n",
       "      <td>301</td>\n",
       "      <td>2012 AVEVA acquires Bocad, adding advanced str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary_32480</td>\n",
       "      <td>Yorkshire Water  Services Limited Annual Repor...</td>\n",
       "      <td>509</td>\n",
       "      <td>Yorkshire Water Services Limited Annual Report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary_30817</td>\n",
       "      <td>Annual Report 201 7 Strategic Report   What we...</td>\n",
       "      <td>699</td>\n",
       "      <td>Annual Report 201 7 Strategic Report What we d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                      extra_summary  \\\n",
       "0  summary_32044  25546.04    9 August 2017  PM    Proof Six NCC...   \n",
       "1  summary_33070  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2  summary_32139  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...   \n",
       "3  summary_32480  Yorkshire Water  Services Limited Annual Repor...   \n",
       "4  summary_30817  Annual Report 201 7 Strategic Report   What we...   \n",
       "\n",
       "   token_count                                    gen_abs_summary  \n",
       "0          780  25546.04 9 August 2017 PM Proof Six NCC Group ...  \n",
       "1          666  Tellitstraight Reuters Annual Report and Form ...  \n",
       "2          301  2012 AVEVA acquires Bocad, adding advanced str...  \n",
       "3          509  Yorkshire Water Services Limited Annual Report...  \n",
       "4          699  Annual Report 201 7 Strategic Report What we d...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ext_summ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75ca79da",
   "metadata": {
    "id": "75ca79da"
   },
   "outputs": [],
   "source": [
    "# Assuming 'res_ext_summ' is your DataFrame\n",
    "res_ext_summ.to_csv('abs_summary_peg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd9c2b64",
   "metadata": {
    "id": "cd9c2b64"
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv('abs_summary_peg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d980e92",
   "metadata": {
    "id": "8d980e92",
    "outputId": "c53a57eb-fb9c-42c9-b399-cd95e23f6cdd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extra_summary</th>\n",
       "      <th>token_count</th>\n",
       "      <th>gen_abs_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summary_32044</td>\n",
       "      <td>25546.04    9 August 2017  PM    Proof Six NCC...</td>\n",
       "      <td>780</td>\n",
       "      <td>25546.04 9 August 2017 PM Proof Six NCC Group ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summary_33070</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>666</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_32139</td>\n",
       "      <td>ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...</td>\n",
       "      <td>301</td>\n",
       "      <td>2012 AVEVA acquires Bocad, adding advanced str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary_32480</td>\n",
       "      <td>Yorkshire Water  Services Limited Annual Repor...</td>\n",
       "      <td>509</td>\n",
       "      <td>Yorkshire Water Services Limited Annual Report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary_30817</td>\n",
       "      <td>Annual Report 201 7 Strategic Report   What we...</td>\n",
       "      <td>699</td>\n",
       "      <td>Annual Report 201 7 Strategic Report What we d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                      extra_summary  \\\n",
       "0  summary_32044  25546.04    9 August 2017  PM    Proof Six NCC...   \n",
       "1  summary_33070  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2  summary_32139  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...   \n",
       "3  summary_32480  Yorkshire Water  Services Limited Annual Repor...   \n",
       "4  summary_30817  Annual Report 201 7 Strategic Report   What we...   \n",
       "\n",
       "   token_count                                    gen_abs_summary  \n",
       "0          780  25546.04 9 August 2017 PM Proof Six NCC Group ...  \n",
       "1          666  Tellitstraight Reuters Annual Report and Form ...  \n",
       "2          301  2012 AVEVA acquires Bocad, adding advanced str...  \n",
       "3          509  Yorkshire Water Services Limited Annual Report...  \n",
       "4          699  Annual Report 201 7 Strategic Report What we d...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "544c80bb",
   "metadata": {
    "id": "544c80bb",
    "outputId": "b7aefc9f-158a-471e-fbd1-0d343b115905"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      summary_32044\n",
       "1      summary_33070\n",
       "2      summary_32139\n",
       "3      summary_32480\n",
       "4      summary_30817\n",
       "           ...      \n",
       "358    summary_32540\n",
       "359    summary_31618\n",
       "360    summary_31037\n",
       "361    summary_32134\n",
       "362    summary_30897\n",
       "Name: filename, Length: 363, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b88b909",
   "metadata": {
    "id": "9b88b909"
   },
   "outputs": [],
   "source": [
    "# Assuming 'x' is your DataFrame\n",
    "x['numeric_part'] = x['filename'].str.extract(r'(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d2ab528",
   "metadata": {
    "id": "9d2ab528",
    "outputId": "8bcf174c-a542-46ed-b3f6-71c2ffa55bb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      32044\n",
       "1      33070\n",
       "2      32139\n",
       "3      32480\n",
       "4      30817\n",
       "       ...  \n",
       "358    32540\n",
       "359    31618\n",
       "360    31037\n",
       "361    32134\n",
       "362    30897\n",
       "Name: numeric_part, Length: 363, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['numeric_part']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736dc091",
   "metadata": {
    "id": "736dc091"
   },
   "source": [
    "# take actual summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88aba58b",
   "metadata": {
    "id": "88aba58b"
   },
   "outputs": [],
   "source": [
    "# Assuming 'x' is your DataFrame\n",
    "folder_path = 'fns2020_dataset/validation/gold_summaries'\n",
    "\n",
    "def read_summary_file(file_name):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Apply the function to each row in 'numeric_part' column and create a new column 'file_content'\n",
    "x['file_content'] = x['numeric_part'].apply(lambda num: read_summary_file(f'{num}_1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7280aa18",
   "metadata": {
    "id": "7280aa18",
    "outputId": "85bb2714-f6fa-4235-effb-943ae525477f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extra_summary</th>\n",
       "      <th>token_count</th>\n",
       "      <th>gen_abs_summary</th>\n",
       "      <th>numeric_part</th>\n",
       "      <th>file_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summary_32044</td>\n",
       "      <td>25546.04    9 August 2017  PM    Proof Six NCC...</td>\n",
       "      <td>780</td>\n",
       "      <td>25546.04 9 August 2017 PM Proof Six NCC Group ...</td>\n",
       "      <td>32044</td>\n",
       "      <td>25546.04    9 August 2017 3:58 PM    Proof Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summary_33070</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>666</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>33070</td>\n",
       "      <td>CHIEF EXECUTIVE’S REVIEW\\nReuters Group PLC A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_32139</td>\n",
       "      <td>ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...</td>\n",
       "      <td>301</td>\n",
       "      <td>2012 AVEVA acquires Bocad, adding advanced str...</td>\n",
       "      <td>32139</td>\n",
       "      <td>14\\nAVEVA GROUP PLC ANNUAL REPORT AND ACCOUNT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary_32480</td>\n",
       "      <td>Yorkshire Water  Services Limited Annual Repor...</td>\n",
       "      <td>509</td>\n",
       "      <td>Yorkshire Water Services Limited Annual Report...</td>\n",
       "      <td>32480</td>\n",
       "      <td>Chief Executive’s Overview\\nGood progress\\nTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary_30817</td>\n",
       "      <td>Annual Report 201 7 Strategic Report   What we...</td>\n",
       "      <td>699</td>\n",
       "      <td>Annual Report 201 7 Strategic Report What we d...</td>\n",
       "      <td>30817</td>\n",
       "      <td>Strategic Report  \\nQ&amp;A with Interim Group Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                      extra_summary  \\\n",
       "0  summary_32044  25546.04    9 August 2017  PM    Proof Six NCC...   \n",
       "1  summary_33070  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2  summary_32139  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...   \n",
       "3  summary_32480  Yorkshire Water  Services Limited Annual Repor...   \n",
       "4  summary_30817  Annual Report 201 7 Strategic Report   What we...   \n",
       "\n",
       "   token_count                                    gen_abs_summary  \\\n",
       "0          780  25546.04 9 August 2017 PM Proof Six NCC Group ...   \n",
       "1          666  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2          301  2012 AVEVA acquires Bocad, adding advanced str...   \n",
       "3          509  Yorkshire Water Services Limited Annual Report...   \n",
       "4          699  Annual Report 201 7 Strategic Report What we d...   \n",
       "\n",
       "  numeric_part                                       file_content  \n",
       "0        32044   25546.04    9 August 2017 3:58 PM    Proof Si...  \n",
       "1        33070   CHIEF EXECUTIVE’S REVIEW\\nReuters Group PLC A...  \n",
       "2        32139   14\\nAVEVA GROUP PLC ANNUAL REPORT AND ACCOUNT...  \n",
       "3        32480   Chief Executive’s Overview\\nGood progress\\nTh...  \n",
       "4        30817   Strategic Report  \\nQ&A with Interim Group Ch...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c4b3e0d",
   "metadata": {
    "id": "1c4b3e0d",
    "outputId": "68dac612-1548-465d-cf23-b847820f04d2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extra_summary</th>\n",
       "      <th>token_count</th>\n",
       "      <th>gen_abs_summary</th>\n",
       "      <th>numeric_part</th>\n",
       "      <th>file_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summary_32044</td>\n",
       "      <td>25546.04    9 August 2017  PM    Proof Six NCC...</td>\n",
       "      <td>780</td>\n",
       "      <td>25546.04 9 August 2017 PM Proof Six NCC Group ...</td>\n",
       "      <td>32044</td>\n",
       "      <td>25546.04    9 August 2017 3:58 PM    Proof Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summary_33070</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>666</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>33070</td>\n",
       "      <td>CHIEF EXECUTIVE’S REVIEW\\nReuters Group PLC A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_32139</td>\n",
       "      <td>ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...</td>\n",
       "      <td>301</td>\n",
       "      <td>2012 AVEVA acquires Bocad, adding advanced str...</td>\n",
       "      <td>32139</td>\n",
       "      <td>14\\nAVEVA GROUP PLC ANNUAL REPORT AND ACCOUNT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary_32480</td>\n",
       "      <td>Yorkshire Water  Services Limited Annual Repor...</td>\n",
       "      <td>509</td>\n",
       "      <td>Yorkshire Water Services Limited Annual Report...</td>\n",
       "      <td>32480</td>\n",
       "      <td>Chief Executive’s Overview\\nGood progress\\nTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary_30817</td>\n",
       "      <td>Annual Report 201 7 Strategic Report   What we...</td>\n",
       "      <td>699</td>\n",
       "      <td>Annual Report 201 7 Strategic Report What we d...</td>\n",
       "      <td>30817</td>\n",
       "      <td>Strategic Report  \\nQ&amp;A with Interim Group Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>summary_32540</td>\n",
       "      <td>HAYWARD TYLER GROUP PLC     REPORT &amp; ACCOUNTS ...</td>\n",
       "      <td>381</td>\n",
       "      <td>Markets we serve Power Generation: Oil &amp; Gas: ...</td>\n",
       "      <td>32540</td>\n",
       "      <td>Hayward Tyler Group PLC \\nFinancial statement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>summary_31618</td>\n",
       "      <td>ITE Group plc  Annual Report and Accounts 2017...</td>\n",
       "      <td>574</td>\n",
       "      <td>3 Like-for-like results are stated on a consta...</td>\n",
       "      <td>31618</td>\n",
       "      <td>Financial statements Governance Strategic rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>summary_31037</td>\n",
       "      <td>ANNUAL REPORT Symphony Environmental Technolog...</td>\n",
       "      <td>403</td>\n",
       "      <td>ANNUAL REPORT Symphony Environmental Technolog...</td>\n",
       "      <td>31037</td>\n",
       "      <td>Chief Executive’s Review\\nMichael Laurier\\nSy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>summary_32134</td>\n",
       "      <td>ANNUAL REPORT AND   FINANCIAL STATEMENTS  2016...</td>\n",
       "      <td>507</td>\n",
       "      <td>Group underlying sales* Profit before tax £12....</td>\n",
       "      <td>32134</td>\n",
       "      <td>STRATEGIC REPORT\\nChief Executive's review\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>summary_30897</td>\n",
       "      <td>Ubisense Group plc Annual Report 2017 Ubisense...</td>\n",
       "      <td>419</td>\n",
       "      <td>Our solutions are based on powerful enterprise...</td>\n",
       "      <td>30897</td>\n",
       "      <td>Bringing the \\ndigital twin to life\\nWe have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                      extra_summary  \\\n",
       "0    summary_32044  25546.04    9 August 2017  PM    Proof Six NCC...   \n",
       "1    summary_33070  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2    summary_32139  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...   \n",
       "3    summary_32480  Yorkshire Water  Services Limited Annual Repor...   \n",
       "4    summary_30817  Annual Report 201 7 Strategic Report   What we...   \n",
       "..             ...                                                ...   \n",
       "358  summary_32540  HAYWARD TYLER GROUP PLC     REPORT & ACCOUNTS ...   \n",
       "359  summary_31618  ITE Group plc  Annual Report and Accounts 2017...   \n",
       "360  summary_31037  ANNUAL REPORT Symphony Environmental Technolog...   \n",
       "361  summary_32134  ANNUAL REPORT AND   FINANCIAL STATEMENTS  2016...   \n",
       "362  summary_30897  Ubisense Group plc Annual Report 2017 Ubisense...   \n",
       "\n",
       "     token_count                                    gen_abs_summary  \\\n",
       "0            780  25546.04 9 August 2017 PM Proof Six NCC Group ...   \n",
       "1            666  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2            301  2012 AVEVA acquires Bocad, adding advanced str...   \n",
       "3            509  Yorkshire Water Services Limited Annual Report...   \n",
       "4            699  Annual Report 201 7 Strategic Report What we d...   \n",
       "..           ...                                                ...   \n",
       "358          381  Markets we serve Power Generation: Oil & Gas: ...   \n",
       "359          574  3 Like-for-like results are stated on a consta...   \n",
       "360          403  ANNUAL REPORT Symphony Environmental Technolog...   \n",
       "361          507  Group underlying sales* Profit before tax £12....   \n",
       "362          419  Our solutions are based on powerful enterprise...   \n",
       "\n",
       "    numeric_part                                       file_content  \n",
       "0          32044   25546.04    9 August 2017 3:58 PM    Proof Si...  \n",
       "1          33070   CHIEF EXECUTIVE’S REVIEW\\nReuters Group PLC A...  \n",
       "2          32139   14\\nAVEVA GROUP PLC ANNUAL REPORT AND ACCOUNT...  \n",
       "3          32480   Chief Executive’s Overview\\nGood progress\\nTh...  \n",
       "4          30817   Strategic Report  \\nQ&A with Interim Group Ch...  \n",
       "..           ...                                                ...  \n",
       "358        32540   Hayward Tyler Group PLC \\nFinancial statement...  \n",
       "359        31618   Financial statements Governance Strategic rep...  \n",
       "360        31037   Chief Executive’s Review\\nMichael Laurier\\nSy...  \n",
       "361        32134   STRATEGIC REPORT\\nChief Executive's review\\nI...  \n",
       "362        30897   Bringing the \\ndigital twin to life\\nWe have ...  \n",
       "\n",
       "[363 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50768768",
   "metadata": {
    "id": "50768768",
    "outputId": "8ad3dd80-3a74-45eb-cd43-5eb051fc513f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2002 Annual Report& Accounts DICOM Group plc Leading in Products and Services for the Electronic Document Capture Market and the Automation of Related Business Transactions DOC UMENT S IN INF ORMA TION OUT 13 39 14 18 20 23 6 DICOM Group plc 2 Annual Report 2002 3 4 About DICOM Group 5 Financial Highlights 6 Chairman’s Statement 9 Chief Executive s Review 11 Financial Review 13 Making Your Information Digital 14 Our Customers 17 Our Global EDC Team 18 Our EDC Products 20 Our Commitment to the Future 21 SGA Division 22 International Presence 23 Team & Organisation 24 Company News 25 The DICOM Group Share 27 Directors 29 Directors’ Report 31 Corporate Governance Statement 34 Remuneration Report 36 Statement of Directors’ Responsibilities 37 Independent Auditors’ Report 39 Consolidated Financial Statements 45 Notes to the Financial Statements 62 Five Year Record 63 Notice of Annual General Meeting 68 Company Secretary and Advisers 68 Principal Subsidiaries C ONTENT S DICOM Group plc 4 DICOM Group plc DICOM Group is the global leader in the Electronic Document Capture (EDC) market, a fast growing segment of the Office Automation industry.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['gen_abs_summary'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d7b0232",
   "metadata": {
    "id": "8d7b0232",
    "outputId": "254af94a-b50c-457f-e7df-387f9c41514b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Annual Report 2002\\n9\\nI am delighted to provide a review of the trading\\ncompanies in my new position as Chief\\nExecutive of DICOM Group. Since being founded\\n11 years ago and as a result of the vision and\\nleadership provided by my predecessor Otto\\nSchmid, the Group has shown an exceptional\\ndevelopment. We were established as a Swiss\\ncompany trading IT peripherals and have gone\\nthrough a substantial evolution since. Today, we\\nare recognised as the worldwide leader in our\\ntarget markets, active in over 60 countries\\naround the globe.\\nIn the past most enterprises have heavily\\ninvested in a broad range of IT solutions that\\nautomate their business transactions and\\nsupport decision making. However connectivity\\nbetween input devices and applications is\\nfrequently inadequate or absent resulting in\\ndata and document access being imprecise,\\nslow and hence expensive.\\nThe EDC solutions offered by DICOM Group\\nsolve this dilemma at an attractive cost to\\nbenefit ratio. We have been able to grow the\\nEDC Division by approximately 30% year over\\nyear since our flotation in 1996, through our\\ntechnological leadership and strict adherence to\\nour successful sales channel concept.  Building\\non this success it is my goal to improve the\\nGroup’s market position in the EDC market,\\nwhilst also adding businesses that are\\ncomplementary to our core area of activities.\\nDICOM Group achieved excellent results in its\\ncore EDC Division last year in contrast to\\nfurther deteriorating conditions in the IT\\nindustry. Our business partners and customers\\nhave continued to report that IT budgets remain\\nunder tight scrutiny. EDC investments, however,\\nwere broadly saved from this development as\\nthey allow quick payback periods to be\\nachieved.\\nEDC now contributes more than 68% to total\\nGroup turnover. Turnover with own products\\nand services showed substantial growth during\\nthe year and contributed 39% and 17%\\nrespectively of sales. Third party products sold\\nby the EDC Division accounted for the balance.\\nIt is our strategy to leverage our position in the\\nEDC market to make DICOM Group’s products\\navailable to the high and low end of the EDC\\nmarket. As a result we have allocated\\nsignificantly more funds to research and\\ndevelopment.  Last year we increased spending\\nby 41% to £7.4m (2001: £5.3m). This\\ncommitment has allowed us a high rate of\\nproduct innovations for existing products and\\nthe launch of entirely new products at the same\\ntime.\\nPRODUCT DEVELOPMENTS\\nOur award winning Virtual ReScan™ (VRS)\\nproduct line, which optimises both image quality\\nand the scan process, continues to do very well.\\nTurnover development in £m since IPO\\nand number of employees\\n160\\n140\\n120\\n100\\n80\\n60\\n40\\n20\\n96\\n*\\n97 98 99 00 01 02\\nturnover                 Number of employees          \\n* \\npro forma unaudited\\n117\\n250\\n320\\n339\\n486\\n718\\n773\\nCHIEF EXE C UTIVE’ S\\nREVIEW which is underscored by the successful\\nparticipation in a rising number of large EDC\\nprojects. Consequently the number of Ascent\\nCapture software licences sold to date has\\nincreased significantly to 32,500 (30 June 2001:\\n22,500). \\nEUROPE\\nDICOM Europe, our European sales and service\\noperations, provided solid growth and better\\nmargins. In the Group’s major territories an\\nimproved performance was achieved in contrast\\nto the general slow-down in the IT industry. The\\ndrive to provide consulting and other services\\ncontinues to improve the quantity and quality of\\nthe Group’s service revenues.\\nASIA\\nIn June we announced that our stake in DICOM\\nAsia was increased from 40% to 80%. This\\nacquisition strengthens our involvement in\\nSouth East Asia, the Greater China region and\\nAustralia and reinforces the Group’s ongoing\\nstrategy to grow both organically and by\\nacquisitions. During the past four years, DICOM\\nAsia has successfully managed to expand its\\nreach, customer base and capabilities in very\\nchallenging market conditions. We are currently\\nincreasing our activities in the Greater China\\nregion and also intend to exploit fully our leading\\nposition in the high growth Asian market.\\nDICOM Group is better placed than ever to\\ncapitalise on its increased global presence, with\\nsales and support services in every major\\nmarket in the world and a product range which\\nis the most comprehensive in the industry, at the\\ncutting edge of today’s technology in EDC\\nsoftware.\\nArnold von Büren\\nChief Executive Officer 10 October 2002\\nDICOM Group plc\\n10\\nThe software version of VRS (v2.1) now supports\\nmore than 40 document scanner models with\\nreal time image enhancement features. Several\\nlarger projects were secured. The ASIC version\\nof VRS, which is being sold on the basis of OEM\\nagreement for use in document scanners, is\\nbeing sold to both new scanner manufacturers\\nand for new scanner models thereby increasing\\nits future sales prospects significantly. In May\\n2002 we were awarded an US patent for VRS.\\nAscent Capture 5.5, a major release of the award\\nwinning capture application, was launched in\\nMay 2002 offering broader product functionality\\nand improved support for high performance\\ndatabases. With these improvements Ascent\\nCapture has been able to gain further market\\nshare in the high end sector of the EDC market.\\nIn June 2002, we launched Ascent Ricochet. It is\\nthe first software product on the market that\\nextends professional EDC features to every\\ndesktop in an enterprise, government\\norganisation or not for profit organisation. It\\npermits Multifunctional Products and digital\\ncopiers to be used as document scanners.\\nAlmost all organisations have copiers and\\nscanner in use.  This new product brings added\\nefficiency and features to these equipment\\ninvestments.\\nBlue chip software houses and system\\nintegrators have continued to integrate Ascent\\nCapture products and VRS into their solutions as\\na result of ongoing focused sales and marketing\\ninitiatives. Ascent Capture, also described in the\\nEDC market as “The Operating System of\\nCapture”, continues to move upmarket – a trend\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['file_content'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4bf1fbd",
   "metadata": {
    "id": "a4bf1fbd",
    "outputId": "b2c3bec3-7cd3-470e-ed65-5d0f06afe8b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extra_summary</th>\n",
       "      <th>token_count</th>\n",
       "      <th>gen_abs_summary</th>\n",
       "      <th>numeric_part</th>\n",
       "      <th>file_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summary_32044</td>\n",
       "      <td>25546.04    9 August 2017  PM    Proof Six NCC...</td>\n",
       "      <td>780</td>\n",
       "      <td>25546.04 9 August 2017 PM Proof Six NCC Group ...</td>\n",
       "      <td>32044</td>\n",
       "      <td>25546.04    9 August 2017 3:58 PM    Proof Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summary_33070</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>666</td>\n",
       "      <td>Tellitstraight Reuters Annual Report and Form ...</td>\n",
       "      <td>33070</td>\n",
       "      <td>CHIEF EXECUTIVE’S REVIEW\\nReuters Group PLC A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_32139</td>\n",
       "      <td>ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...</td>\n",
       "      <td>301</td>\n",
       "      <td>2012 AVEVA acquires Bocad, adding advanced str...</td>\n",
       "      <td>32139</td>\n",
       "      <td>14\\nAVEVA GROUP PLC ANNUAL REPORT AND ACCOUNT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary_32480</td>\n",
       "      <td>Yorkshire Water  Services Limited Annual Repor...</td>\n",
       "      <td>509</td>\n",
       "      <td>Yorkshire Water Services Limited Annual Report...</td>\n",
       "      <td>32480</td>\n",
       "      <td>Chief Executive’s Overview\\nGood progress\\nTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary_30817</td>\n",
       "      <td>Annual Report 201 7 Strategic Report   What we...</td>\n",
       "      <td>699</td>\n",
       "      <td>Annual Report 201 7 Strategic Report What we d...</td>\n",
       "      <td>30817</td>\n",
       "      <td>Strategic Report  \\nQ&amp;A with Interim Group Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                      extra_summary  \\\n",
       "0  summary_32044  25546.04    9 August 2017  PM    Proof Six NCC...   \n",
       "1  summary_33070  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2  summary_32139  ANNUAL REPORT AND ACCOUNTS 2017 50 YEARS OF  I...   \n",
       "3  summary_32480  Yorkshire Water  Services Limited Annual Repor...   \n",
       "4  summary_30817  Annual Report 201 7 Strategic Report   What we...   \n",
       "\n",
       "   token_count                                    gen_abs_summary  \\\n",
       "0          780  25546.04 9 August 2017 PM Proof Six NCC Group ...   \n",
       "1          666  Tellitstraight Reuters Annual Report and Form ...   \n",
       "2          301  2012 AVEVA acquires Bocad, adding advanced str...   \n",
       "3          509  Yorkshire Water Services Limited Annual Report...   \n",
       "4          699  Annual Report 201 7 Strategic Report What we d...   \n",
       "\n",
       "  numeric_part                                       file_content  \n",
       "0        32044   25546.04    9 August 2017 3:58 PM    Proof Si...  \n",
       "1        33070   CHIEF EXECUTIVE’S REVIEW\\nReuters Group PLC A...  \n",
       "2        32139   14\\nAVEVA GROUP PLC ANNUAL REPORT AND ACCOUNT...  \n",
       "3        32480   Chief Executive’s Overview\\nGood progress\\nTh...  \n",
       "4        30817   Strategic Report  \\nQ&A with Interim Group Ch...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "851d6e29",
   "metadata": {
    "id": "851d6e29"
   },
   "outputs": [],
   "source": [
    "# Assuming 'res_ext_summ' is your DataFrame\n",
    "x.to_csv('final_abs_summary_peg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsKDbn-Br2SY",
   "metadata": {
    "id": "LsKDbn-Br2SY"
   },
   "source": [
    "# Create Txt Files From CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a85282cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_summ = pd.read_csv('final_abs_summary_peg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31320a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extra_summary</th>\n",
       "      <th>token_count</th>\n",
       "      <th>gen_abs_summary</th>\n",
       "      <th>numeric_part</th>\n",
       "      <th>file_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>summary_32540</td>\n",
       "      <td>HAYWARD TYLER GROUP PLC     REPORT &amp; ACCOUNTS ...</td>\n",
       "      <td>381</td>\n",
       "      <td>Markets we serve Power Generation: Oil &amp; Gas: ...</td>\n",
       "      <td>32540</td>\n",
       "      <td>Hayward Tyler Group PLC \\nFinancial statement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>summary_31618</td>\n",
       "      <td>ITE Group plc  Annual Report and Accounts 2017...</td>\n",
       "      <td>574</td>\n",
       "      <td>3 Like-for-like results are stated on a consta...</td>\n",
       "      <td>31618</td>\n",
       "      <td>Financial statements Governance Strategic rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>summary_31037</td>\n",
       "      <td>ANNUAL REPORT Symphony Environmental Technolog...</td>\n",
       "      <td>403</td>\n",
       "      <td>ANNUAL REPORT Symphony Environmental Technolog...</td>\n",
       "      <td>31037</td>\n",
       "      <td>Chief Executive’s Review\\nMichael Laurier\\nSy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>summary_32134</td>\n",
       "      <td>ANNUAL REPORT AND   FINANCIAL STATEMENTS  2016...</td>\n",
       "      <td>507</td>\n",
       "      <td>Group underlying sales* Profit before tax £12....</td>\n",
       "      <td>32134</td>\n",
       "      <td>STRATEGIC REPORT\\nChief Executive's review\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>summary_30897</td>\n",
       "      <td>Ubisense Group plc Annual Report 2017 Ubisense...</td>\n",
       "      <td>419</td>\n",
       "      <td>Our solutions are based on powerful enterprise...</td>\n",
       "      <td>30897</td>\n",
       "      <td>Bringing the \\ndigital twin to life\\nWe have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                      extra_summary  \\\n",
       "358  summary_32540  HAYWARD TYLER GROUP PLC     REPORT & ACCOUNTS ...   \n",
       "359  summary_31618  ITE Group plc  Annual Report and Accounts 2017...   \n",
       "360  summary_31037  ANNUAL REPORT Symphony Environmental Technolog...   \n",
       "361  summary_32134  ANNUAL REPORT AND   FINANCIAL STATEMENTS  2016...   \n",
       "362  summary_30897  Ubisense Group plc Annual Report 2017 Ubisense...   \n",
       "\n",
       "     token_count                                    gen_abs_summary  \\\n",
       "358          381  Markets we serve Power Generation: Oil & Gas: ...   \n",
       "359          574  3 Like-for-like results are stated on a consta...   \n",
       "360          403  ANNUAL REPORT Symphony Environmental Technolog...   \n",
       "361          507  Group underlying sales* Profit before tax £12....   \n",
       "362          419  Our solutions are based on powerful enterprise...   \n",
       "\n",
       "     numeric_part                                       file_content  \n",
       "358         32540   Hayward Tyler Group PLC \\nFinancial statement...  \n",
       "359         31618   Financial statements Governance Strategic rep...  \n",
       "360         31037   Chief Executive’s Review\\nMichael Laurier\\nSy...  \n",
       "361         32134   STRATEGIC REPORT\\nChief Executive's review\\nI...  \n",
       "362         30897   Bringing the \\ndigital twin to life\\nWe have ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_summ.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48ffed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(abs_summ)\n",
    "output_directory = 'output_abs_summaries'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save each row's \"gen_abs_summary\" into a separate text file\n",
    "for index, row in df.iterrows():\n",
    "    numeric_part = row['numeric_part']\n",
    "    summary_text = row['gen_abs_summary']\n",
    "\n",
    "    # Creating the file name\n",
    "    file_name = os.path.join(output_directory, f'{numeric_part}_summ.txt')\n",
    "\n",
    "    # Saving the text to the file\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1ef69",
   "metadata": {},
   "source": [
    "# Calculate Average Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05aa3c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average for ROUGE-L:\n",
      "Avg_Recall       0.132869\n",
      "Avg_Precision    0.481902\n",
      "Avg_F-Score      0.193183\n",
      "dtype: float64\n",
      "\n",
      "Average for ROUGE-1:\n",
      "Avg_Recall       0.152009\n",
      "Avg_Precision    0.509783\n",
      "Avg_F-Score      0.205946\n",
      "dtype: float64\n",
      "\n",
      "Average for ROUGE-2:\n",
      "Avg_Recall       0.082421\n",
      "Avg_Precision    0.190772\n",
      "Avg_F-Score      0.094931\n",
      "dtype: float64\n",
      "\n",
      "Average for ROUGE-SU4:\n",
      "Avg_Recall       0.119188\n",
      "Avg_Precision    0.224869\n",
      "Avg_F-Score      0.129694\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Assuming your data is in a CSV file named 'your_dataset.csv'\n",
    "# If it's in a different format, adjust the read function accordingly\n",
    "df = pd.read_csv('results.csv')\n",
    "\n",
    "# Separate data for each ROUGE category\n",
    "rouge_l_data = df[df['ROUGE-Type'] == 'ROUGE-L+StopWordRemoval+Stemming']\n",
    "rouge_1_data = df[df['ROUGE-Type'] == 'ROUGE-1+StopWordRemoval+Stemming']\n",
    "rouge_2_data = df[df['ROUGE-Type'] == 'ROUGE-2+StopWordRemoval+Stemming']\n",
    "rouge_su4_data = df[df['ROUGE-Type'] == 'ROUGE-SU4+StopWordRemoval+Stemming']\n",
    "\n",
    "# Calculate averages for each category\n",
    "avg_rouge_l = rouge_l_data[['Avg_Recall', 'Avg_Precision', 'Avg_F-Score']].mean()\n",
    "avg_rouge_1 = rouge_1_data[['Avg_Recall', 'Avg_Precision', 'Avg_F-Score']].mean()\n",
    "avg_rouge_2 = rouge_2_data[['Avg_Recall', 'Avg_Precision', 'Avg_F-Score']].mean()\n",
    "avg_rouge_su4 = rouge_su4_data[['Avg_Recall', 'Avg_Precision', 'Avg_F-Score']].mean()\n",
    "\n",
    "# Print the results with labels\n",
    "print(\"Average for ROUGE-L:\")\n",
    "print(avg_rouge_l)\n",
    "\n",
    "print(\"\\nAverage for ROUGE-1:\")\n",
    "print(avg_rouge_1)\n",
    "\n",
    "print(\"\\nAverage for ROUGE-2:\")\n",
    "print(avg_rouge_2)\n",
    "\n",
    "print(\"\\nAverage for ROUGE-SU4:\")\n",
    "print(avg_rouge_su4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
